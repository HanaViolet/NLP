这份自我评价报告剔除了与课程核心关联度不大的DSL内容，紧扣你提供的 **CP1-CP12 考点**，并将 **SwimVG 论文** 作为掌握 **CP7（多模态）** 和 **CP9（微调）** 的核心实战证据。

文中已为你预留了**填写证据（如代码文件、笔记截图、推导手稿）**的空白处。

---

# 自然语言处理 (NLP) 课程自我评价报告

**学生姓名**：[你的名字]
**日期**：2025年12月31日
**核心成果**：N-gram平滑算法实现、NumPy手写神经网络、SwimVG多模态复现

---

## 一、 综述：从统计概率到大模型范式

本学期，我紧紧围绕课程大纲，构建了从**统计语言模型**（CP3）到**深度学习序列模型**（CP4/CP5），再到现代**Transformer与大模型**（CP6/CP11）的完整知识体系。

在理论学习之外，我特别注重底层算法的推导（如手动推导反向传播），并通过复现前沿论文《SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding》，深入实践了 **CLIP (CP7)** 和 **参数高效微调 (CP9)** 等高阶考点。

---

## 二、 阶段一：统计基础与词向量 (CP3, CP4)

课程初期，我着重理解了 NLP 的数学基础，特别是如何处理稀疏数据和语义表示。

1. **N-gram 与平滑技术（CP3 重点）**：
* 针对 N-gram 模型中常见的零概率问题，我深入推导了**加一平滑（Add-one Smoothing）**公式。
* 我理解了平滑处理不仅是数学修正，更是解决数据稀疏性的关键手段，这为后续理解 CP11 中 Scaling Law（缩放法则）与数据覆盖率的关系打下了直观基础。
* **证据**：`[____________________]` *(此处请填入：N-gram平滑算法的代码片段 / 概率计算笔记)*


2. **词向量表示与计算（CP4 基础）**：
* 我对比了 **CBOW** 和 **Skip-gram** 的训练目标差异，理解了 Word2Vec 如何通过上下文预测来捕捉词汇的语义关系。
* 此外，我复习了 RNN 在语言模型中的应用，掌握了基于时间步的更新公式，并分析了其在长序列建模中的局限性。



---

## 三、 阶段二：神经网络底层与序列模型 (CP4, CP5)

为了彻底弄懂 CP5 中“为什么原始 RNN 有问题”，我选择了一条“去框架化”的硬核学习路径。

1. **纯 NumPy 手写反向传播（CP4/CP5 难点）**：
* 我挑战了不使用 PyTorch，仅用 **NumPy 手写神经网络的反向传播（Backpropagation）**。
* 通过亲手推导矩阵的链式法则，我直观地观察到了**梯度消失（Vanishing Gradient）**现象。这一实践完美解释了 CP5 的核心问题：为什么传统 RNN 难以捕捉长距离依赖，以及为何 LSTM 和 Transformer 是必然的选择。
* **证据**：`[____________________]` *(此处请填入：纯NumPy手写神经网络代码 / 梯度推导手稿)*


2. **Seq2Seq 与训练策略（CP5 核心）**：
* 我详细梳理了 Seq2Seq 架构，重点对比了训练时的 **Teacher Forcing** 模式与推理时的 **Free-running** 模式的区别。
* 在解码算法上，我分析了 **Beam Search** 相较于贪心搜索（Greedy Search）在生成质量上的优势。



---

## 四、 阶段三：Transformer 架构与 LLM 演进 (CP5, CP6, CP11)

这是本学期最核心的模块，我重点攻克了 Transformer 的内部机制及 LLM 的演变。

1. **Attention 机制与位置编码（CP5/CP6）**：
* **Self-Attention**：我手动推导了  的计算过程，理解了其并行计算能力优于 RNN 的本质。
* **位置编码（RoPE）**：针对 CP6 的考点，我专门研究了**旋转位置编码（Rotary Positional Embedding, RoPE）**。我理解了它如何通过复数域的旋转操作，使模型能够自然地捕捉相对位置信息，这是现代 LLM（如 LLaMA, PaLM）的关键技术。
* **证据**：`[____________________]` *(此处请填入：RoPE理论推导笔记 / Attention机制图解)*


2. **GPT 家族演进（CP6/CP11）**：
* 我梳理了从 GPT-1 到 GPT-3 的架构迭代，结合 **CP11（缩放法则）**，分析了模型参数量、数据量与计算资源之间的幂律关系。



---

## 五、 阶段四：多模态与高效微调实战 —— 基于 SwimVG (CP7, CP9)

为了将理论转化为前沿实战能力，我深入研究了论文《SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding》，以此作为掌握 CP7 和 CP9 的综合证明。

### 1. 预训练模型与多模态对齐 (对应 CP7: ViT/CLIP)

SwimVG 的架构完美体现了 CP7 中关于 **CLIP** 和 **ViT** 的应用：

* 
**架构理解**：该模型冻结了预训练的 **CLIP-B** 作为文本编码器，并使用 **DINOv2**（一种自监督的 ViT） 作为视觉主干。


* 
**深度认知**：论文指出了传统方法直接堆叠 Transformer 进行融合的低效性 。SwimVG 通过**Step-wise Multimodal Prompts (Swip)**  将文本语义逐步注入视觉层，这种 Token 级别的逐步对齐加深了我对多模态交互机制的理解。


* **证据**：`[____________________]` *(此处请填入：SwimVG 模型架构解析图 / 预训练模型加载代码)*

### 2. 参数高效微调 (PETL) 优于全量微调 (对应 CP9: LoRA/Fine-tuning)

针对 CP9 中关于微调算法的考察，我通过 SwimVG 的实验数据验证了 PETL 的优势：

* 
**效率对比**：相比于全量微调（Full Fine-tuning），SwimVG 减少了 **97.96%** 的可训练参数，仅需更新 **2.04%** 的参数 ，同时推理速度比传统方法快约 40% 。这直观地展示了 PETL 解决了“大模型训练成本过高”的问题。


* 
**与 LoRA 的对比**：我对比了 CP9 中的 **LoRA** 与论文提出的 **CIA (Cross-modal Interactive Adapter)**。论文实验显示，在视觉定位任务中，SwimVG 的准确率优于直接使用 LoRA 。


* 
**核心思考**：这让我明白，虽然 LoRA 通用性强，但在多模态任务中，设计专门的**跨模态交互模块（如 CIA）**  能比单纯的低秩矩阵分解带来更好的对齐效果。


* **证据**：`[____________________]` *(此处请填入：SwimVG与LoRA对比实验数据表 / 复现代码中的Adapter模块截图)*

---

## 六、 总结

本学期，我从 N-gram 的平滑公式推导起步，深入到 RNN 梯度的底层计算，最终通过复现顶会论文《SwimVG》串联了 Transformer、CLIP (CP7) 和高效微调 (CP9) 等高阶知识点。我对 NLP 核心架构的演进逻辑（从统计 -> RNN -> Transformer）有了深刻理解，并具备了将 RoPE、Attention 及 PETL 等技术应用于实际项目的能力。
