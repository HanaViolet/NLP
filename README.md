## 评价依据：分周学习记录

### 第一阶段：基础构建与工具熟悉（第1-4周）

#### 第1周
* **学习内容**：初识NLP基本概念，环境配置。
* **具体成果**：
    * 配置了基于 Conda 的 Python 开发环境，解决了早期的包依赖冲突问题。
    * 复习了 Python 数据分析基础（Pandas, Matplotlib），为后续处理文本数据集做准备。
    * **[代码/笔记]**：提交了环境配置文件 `environment.yml` 和基础的 Python 练习代码。

#### 第2周
* **学习内容**：文本预处理与传统特征工程。
* **具体成果**：
    * 学习了正则表达式（Regex）在文本清洗中的应用。
    * 理解了 Tokenization（分词）的概念，对比了中文分词（如 Jieba）与英文分词的区别。
    * **[探索]**：尝试处理了 CSV 格式的非结构化文本数据，进行了初步的数据清洗。

#### 第3周
* **学习内容**：词向量表示（Word Embeddings）。
* **具体成果**：
    * 深入学习了 One-hot 编码的局限性，理解了分布式表示（Distributed Representation）的优势。
    * 研读了 Word2Vec (Skip-gram/CBOW) 和 GloVe 的基本原理。
    * **[笔记]**：整理了关于“词向量如何捕捉语义关系（如 King - Man + Woman = Queen）”的学习笔记。

#### 第4周
* **学习内容**：机器学习在NLP中的应用基础。
* **具体成果**：
    * 复习了分类算法（如 XGBoost, Decision Tree），思考其在文本分类任务中的应用可能性。
    * **[实践]**：结合当时正在进行的“算法疲劳”问卷研究，思考了如何对开放式问卷文本进行分类编码。

---

### 第二阶段：句法分析与形式语言（第5-9周）
*注：此阶段结合了你对Lark和DSL的深入探索，这是非常亮眼的加分项。*

#### 第5周
* **学习内容**：形式语言与自动机理论基础。
* **具体成果**：
    * 学习了上下文无关文法（CFG）和巴科斯范式（BNF/EBNF）。
    * 理解了有限状态自动机（FSA）在简单的文本模式匹配中的作用。

#### 第6周
* **学习内容**：句法分析算法（Parsing）。
* **具体成果**：
    * 对比了自顶向下和自底向上的解析策略。
    * 研究了 Earley Parser 和 LALR(1) 算法的区别与适用场景。

#### 第7-8周（重点突破）
* **学习内容**：领域特定语言（DSL）的设计与实现。
* **具体成果**：
    * **[工程实践]**：使用 Python 的 `Lark` 库设计并实现了一个自定义 DSL 解析器。
    * **[深度探索]**：编写了具体的 EBNF 语法规则文件，解决了二义性文法冲突问题。
    * **[代码]**：提交了完整的 DSL 解析器 Demo，包含语法树（AST）的生成与遍历逻辑（见仓库 `dsl-parser` 目录）。

#### 第9周
* **学习内容**：期中总结与拓展阅读。
* **具体成果**：
    * 阅读了关于从规则系统到统计模型演变的文献。

---

### 第三阶段：深度学习与大模型原理（第10-14周）
*注：此阶段结合了你的LLM Presentation和Transformer研究。*

#### 第10周
* **学习内容**：神经网络基础与反向传播。
* **具体成果**：
    * 深入复习了梯度下降算法。
    * **[深度探索]**：不依赖 PyTorch 框架，仅使用 NumPy 推导并实现了全连接层的反向传播逻辑，彻底弄懂了链式法则在矩阵运算中的具体形态。

#### 第11周
* **学习内容**：序列模型（RNN/LSTM）。
* **具体成果**：
    * 学习了循环神经网络处理变长序列的机制。
    * **[跨域应用]**：在原油价格预测项目中，尝试理解了时间序列数据与自然语言序列在处理逻辑上的异同（如滑动窗口、序列依赖）。

#### 第12周
* **学习内容**：Attention 机制与 Transformer 架构。
* **具体成果**：
    * 这是本学期收获最大的一周。彻底弄懂了 Self-Attention 的 $Q, K, V$ 矩阵计算过程。
    * 理解了 Multi-head Attention 如何在不同子空间捕捉语义信息。
    * **[输出]**：制作了一份关于 "Large Language Models and Generative AI" 的详细演示大纲，梳理了从 RNN 到 Transformer 的演进。

#### 第13周
* **学习内容**：预训练模型（BERT/GPT）与迁移学习。
* **具体成果**：
    * 学习了 Masked Language Model (MLM) 和 Next Token Prediction 的训练目标。
    * 调研了 Vision Transformer (ViT) 和 CLIP 模型，理解了 Transformer 架构在多模态领域的通用性。

#### 第14周
* **学习内容**：大模型应用与 Prompt Engineering。
* **具体成果**：
    * 实践了与 LLM（如 Gemini/GPT）的协作编程，学习了如何通过 Prompt 优化生成代码的质量。
    * 了解了 RAG（检索增强生成）的基本概念。

---

### 第四阶段：底层实现与综合复习（第15-17周）
*注：此阶段结合了你最近的NumPy纯手写神经网络作业。*

#### 第15周（核心亮点）
* **学习内容**：去框架化的深度学习实现（No-framework Implementation）。
* **具体成果**：
    * **[代码]**：完成了“纯 NumPy 手写 BP 神经网络”的任务，实现了前向传播、损失计算、反向传播和参数更新的全流程。
    * **[代码]**：手动实现了一个 Autoencoder（自编码器），用于数据的降维和特征压缩。
    * 此过程极大地加深了我对底层张量运算维度的敏感度。

#### 第16周
* **学习内容**：课程复习与项目整理。
* **具体成果**：
    * 整理了本学期的代码仓库，完善了 `README.md` 文档。
    * 复盘了 SwimVG 和其他 Backend 项目中遇到的环境配置问题，总结了 Python 虚拟环境管理的最佳实践。

#### 第17周
* **学习内容**：撰写自评与总结。
* **具体成果**：
    * 回顾全学期，完成了这份详细的自我评价报告。
    * 打包整理了所有证明材料（PDF及代码链接）。
